{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRadius neighbors classification\n===============================\n\nShows the usage of the radius nearest neighbors classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Pablo Marcos Manch\u00f3n\n# License: MIT\n\n# sphinx_gallery_thumbnail_number = 2\n\n\nimport skfda\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom skfda.ml.classification import RadiusNeighborsClassifier\nfrom skfda.misc.metrics import pairwise_distance, lp_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we are going to show the usage of the radius nearest\nneighbors classifier in their functional version, a variation of the K-nearest\nneighbors classifier, where it is used a vote among neighbors within a given\nradius, instead of use the k nearest neighbors.\n\nFirstly, we will construct a toy dataset to show the basic usage of the API.\n\nWe will create two classes of sinusoidal samples, with different locations\nof their phase.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fd1 = skfda.datasets.make_sinusoidal_process(error_std=.0, phase_std=.35,\n                                             random_state=0)\nfd2 = skfda.datasets.make_sinusoidal_process(phase_mean=1.9, error_std=.0,\n                                             random_state=1)\n\nfd1.plot(color='C0')\nfd2.plot(color='C1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the K-nearest neighbor example, we will split the dataset in two\npartitions, for training and test, using the sklearn function\n:func:`sklearn.model_selection.train_test_split`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Concatenate the two classes in the same FDataGrid\nX = fd1.concatenate(fd2)\ny = np.array(15*[0] + 15*[1])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n                                                    shuffle=True, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the multivariate data, the label assigned to a test sample will be the\nmajority class of its neighbors, in this case all the samples in the ball\ncenter in the sample.\n\nIf we use the $\\mathbb{L}^\\infty$ metric, we can visualize a ball\nas a bandwidth with a fixed radius around a function.\n\nThe following figure shows the ball centered in the first sample of the test\npartition.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\n\nsample = X_test[0]\n\nX_train[y_train == 0].plot(color='C0')\nX_train[y_train == 1].plot(color='C1')\nsample.plot(color='red', linewidth=3)\n\nlower = sample - 0.3\nupper = sample + 0.3\n\nplt.fill_between(sample.sample_points[0], lower.data_matrix.flatten(),\n                 upper.data_matrix[0].flatten(),  alpha=.25, color='C1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, all the neighbors in the ball belong to the first class, so\nthis will be the class predicted.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Creation of pairwise distance\nl_inf = pairwise_distance(lp_distance, p=np.inf)\ndistances = l_inf(sample, X_train)[0] # L_inf distances to 'sample'\n\nplt.figure()\n\nX_train[distances <= .3].plot(color='C0')\nsample.plot(color='red', linewidth=3)\n\nplt.fill_between(sample.sample_points[0], lower.data_matrix.flatten(),\n                 upper.data_matrix[0].flatten(),  alpha=.25, color='C1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will fit the classifier :class:`RadiusNeighborsClassifier\n<skfda.ml.classification.RadiusNeighborsClassifier>`, which has a similar API\nthan the sklearn estimator :class:`sklearn.neighbors.RadiusNeighborsClassifier`\nbut accepting :class:`FDataGrid` instead of arrays with multivariate data.\n\nThe vote of the neighbors can be weighted using the paramenter ``weights``.\nIn this case we will weight the vote inversely proportional to the distance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "radius_nn = RadiusNeighborsClassifier(radius=.3,  weights='distance')\nradius_nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can predict labels for the test partition with :meth:`predict`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = radius_nn.predict(X_test)\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we get 100% accuracy, althouth, it is a toy dataset and it does\nnot have much merit.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_score = radius_nn.score(X_test, y_test)\nprint(test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the K-nearest neighbor example, we can use a sklearn metric\napproximately equivalent to the functional $\\mathbb{L}^2$ one,\nbut computationally faster.\n\nWe saw that $\\|f -g \\|_{\\mathbb{L}^2} \\approx \\sqrt{\\bigtriangleup h} \\,\nd_{euclidean}(\\vec{f}, \\vec{g})$ if the samples are equiespaced (or almost).\n\nIn the KNN case, the constant $\\sqrt{\\bigtriangleup h}$ does not matter,\nbut in this case will affect the value of the radius, dividing by\n$\\sqrt{\\bigtriangleup h}$.\n\nIn this dataset $\\bigtriangleup h=0.001$, so, we have to multiply the\nradius by $10$ to achieve the same result.\n\nThe computation using this metric it is 1000 times faster. See the\nK-neighbors classifier example and the API documentation to get detailled\ninformation.\n\nWe obtain 100% accuracy with this metric too.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "radius_nn = RadiusNeighborsClassifier(radius=3, metric='euclidean',\n                                      weights='distance', sklearn_metric=True)\n\n\nradius_nn.fit(X_train, y_train)\n\ntest_score = radius_nn.score(X_test, y_test)\nprint(test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the radius is too small, it is possible to get samples with no neighbors.\nThe classifier will raise and exception in this case.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "radius_nn.set_params(radius=.5) #\u00a0Radius 0.05 in the L2 distance\nradius_nn.fit(X_train, y_train)\n\ntry:\n    radius_nn.predict(X_test)\nexcept ValueError as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A label to these oulier samples can be provided to avoid this problem.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "radius_nn.set_params(outlier_label=2)\nradius_nn.fit(X_train, y_train)\npred = radius_nn.predict(X_test)\n\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This classifier can be used with multivariate funcional data, as surfaces\nor curves in $\\mathbb{R}^N$, if the metric support it too.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}