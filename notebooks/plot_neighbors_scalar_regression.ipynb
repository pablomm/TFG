{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNeighbors Scalar Regression\n===========================\n\nShows the usage of the nearest neighbors regressor with scalar response.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Pablo Marcos Manch\u00f3n\n# License: MIT\n\n# sphinx_gallery_thumbnail_number = 3\n\nimport skfda\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom skfda.ml.regression import KNeighborsScalarRegressor\nfrom skfda.misc.metrics import norm_lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we are going to show the usage of the nearest neighbors\nregressors with scalar response. There is available a K-nn version,\n:class:`KNeighborsScalarRegressor\n<skfda.ml.regression.KNeighborsScalarRegressor>`, and other one based in the\nradius, :class:`RadiusNeighborsScalarRegressor\n<skfda.ml.regression.RadiusNeighborsScalarRegressor>`.\n\nFirstly we will fetch a dataset to show the basic usage.\n\nThe caniadian weather dataset contains the daily temperature and precipitation\nat 35 different locations in Canada averaged over 1960 to 1994.\n\nThe following figure shows the different temperature curves.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = skfda.datasets.fetch_weather()\nfd = data['data']\n\n# TODO: Change this after merge operations-with-images\nfd.axes_labels = None\nX = fd.copy(data_matrix=fd.data_matrix[..., 0])\n\n\nX.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are not interested in the precipitation curves directly,\nas in the case with regression response, we will train a nearest neighbor\nregressor to predict a scalar magnitude.\n\nIn the next figure the precipitation curves are shown.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_func = fd.copy(data_matrix=fd.data_matrix[..., 1])\n\nplt.figure()\ny_func.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will try to predict the total log precipitation, i.e,\n$logPrecTot_i = \\log \\int_0^{365} prec_i(t)dt$ using the temperature\ncurves.\n\nTo obtain the precTot we will calculate the $\\mathbb{L}^1$ norm of\nthe precipitation curves.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prec = norm_lp(y_func, 1)\nlog_prec = np.log(prec)\n\nprint(log_prec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the nearest neighbors classifier examples, we will split the dataset in\ntwo partitions, for training and test, using the sklearn function\n:func:`sklearn.model_selection.train_test_split`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, log_prec, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly we will try make a prediction with the default values of the\nestimator, using 5 neighbors and the $\\mathbb{L}^2$.\n\nWe can fit the :class:`KNeighborsScalarRegressor\n<skfda.ml.regression.KNeighborsScalarRegressor>` in the same way than the\nsklearn estimators. This estimator is an extension of the sklearn\n:class:`sklearn.neighbors.KNeighborsRegressor`, but accepting a\n:class:`FDataGrid <skfda.FDataGrid>` as input instead of an array with\nmultivariate data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsScalarRegressor(weights='distance')\nknn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can predict values for the test partition using :meth:`predict`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = knn.predict(X_test)\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following figure compares the real precipitations with the predicted\nvalues.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.scatter(y_test, pred)\nplt.plot(y_test, y_test)\nplt.xlabel(\"Total log precipitation\")\nplt.ylabel(\"Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can quantify how much variability it is explained by the model with\nthe coefficient of determination $R^2$ of the prediction,\nusing :meth:`score` for that.\n\nThe coefficient $R^2$ is defined as $(1 - u/v)$, where $u$\nis the residual sum of squares $\\sum_i (y_i - y_{pred_i})^ 2$\nand $v$ is the total sum of squares $\\sum_i (y_i - \\bar y )^2$.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = knn.score(X_test, y_test)\nprint(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we obtain a really good aproximation with this naive approach,\nalthough, due to the small number of samples, the results will depend on\nhow the partition was done. In the above case, the explained variation is\ninflated for this reason.\n\nWe will perform cross-validation to test more robustly our model.\n\nAs in the neighbors classifiers examples, we can use a sklearn metric to\napproximate the $\\mathbb{L}^2$ metric between function, but with a much\nlower computational cost.\n\nAlso, we can make a grid search, using\n:class:`sklearn.model_selection.GridSearchCV`, to determine the optimal number\nof neighbors and the best way to weight their votes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "param_grid = {'n_neighbors': np.arange(1, 12, 2),\n              'weights': ['uniform', 'distance']}\n\n\nknn = KNeighborsScalarRegressor(metric='euclidean', sklearn_metric=True)\ngscv = GridSearchCV(knn, param_grid, cv=KFold(shuffle=True, random_state=0))\ngscv.fit(X, log_prec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain that 7 is the optimal number of neighbors, and a lower value of the\n$R^2$ coefficient, but much closer to the real one.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(gscv.best_params_)\nprint(gscv.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More detailed information about the canadian weather dataset can be obtained\nin the following references.\n\n * Ramsay, James O., and Silverman, Bernard W. (2006). Functional Data\n   Analysis, 2nd ed. , Springer, New York.\n\n *  Ramsay, James O., and Silverman, Bernard W. (2002). Applied Functional\n    Data Analysis, Springer, New York\\n'\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}