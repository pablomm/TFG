{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nK-nearest neighbors classification\n==================================\n\nShows the usage of the k-nearest neighbors classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Pablo Marcos Manch\u00f3n\n# License: MIT\n\nimport skfda\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom skfda.ml.classification import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are going to show the usage of the K-nearest neighbors\nclassifier in their functional version, which is a extension of the\nmultivariate one, but using functional metrics between the observations.\n\nFirstly, we are going to fetch a functional data dataset, such as the Berkeley\nGrowth Study. This dataset correspond to the height of several boys and girls\nmeasured until the 18 years of age.\n\nWe will try to predict the sex by using its growth curves.\n\nThe following figure shows the growth curves grouped by sex.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = skfda.datasets.fetch_growth()\nX = data['data']\ny = data['target']\n\nX[y==0].plot(color='C0')\nX[y==1].plot(color='C1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the class labels are stored in an array with 0's in the male\nsamples and 1's in the positions with female ones.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can split the dataset using the sklearn function\n:func:`train_test_split <sklearn.model_selection.train_test_split>`.\n\nWe will use two thirds of the dataset for the training partition and the\nremaining samples for testing.\n\nThe function will return two :class:`FDataGrid <skfda.FDataGrid>`'s,\n``X_train`` and ``X_test`` with the corresponding partitions, and arrays\nwith their class labels.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will fit the classifier\n:class:`KNeighborsClassifier <skfda.ml.classification.KNeighborsClassifier>`\nwith the training partition. This classifier works exactly like the sklearn\nmultivariate classifier\n:class:`KNeighborsClassifier <sklearn.neighbors.KNeighborsClassifier>` , but\nwill accept as input a :class:`FDataGrid` with functional observations instead\nof an array with multivariate data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once it is fitted, we can predict labels for the test samples.\n\nTo predict the label of a test sample, the classifier will calculate the\nk-nearest neighbors and will asign the majority class. By default, it is\nused the $\\mathbb{L}^2$ distance between functions, to determine the\nneighbourhood of a sample, with 5 neighbors.\n\nCan be used any of the functional metrics of the module\n:mod:`skfda.misc.metrics`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = knn.predict(X_test)\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :func:`score` method allows us to calculate the mean accuracy for the test\ndata. In this case we obtained around 96% of accuracy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = knn.score(X_test, y_test)\nprint(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also estimate the probability of membership to the predicted class\nusing :func:`predict_proba`, which will return an array with the\nprobabilities of the classes, in lexicographic order, for each test sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probs = knn.predict_proba(X_test[:5])\nprint(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the sklearn\n:func:`GridSearchCV <sklearn.model_selection.GridSearchCV>` to perform a\ngrid search to select the best hyperparams, using cross-validation.\n\nIn this case, we will vary the number of neighbors between 1 and 11.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# only odd numbers\nparam_grid = {'n_neighbors': np.arange(1, 12, 2)}\n\n\nknn = KNeighborsClassifier()\ngscv = GridSearchCV(knn, param_grid, cv=KFold(shuffle=True, random_state=0))\ngscv.fit(X, y)\n\n\nprint(\"Best params:\", gscv.best_params_)\nprint(\"Best score:\", gscv.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have obtained the greatest mean accuracy using 3 neighbors.  The following\nfigure shows the score depending on the number of neighbors.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.bar(param_grid['n_neighbors'], gscv.cv_results_['mean_test_score'])\n\nplt.xticks(param_grid['n_neighbors'])\nplt.ylabel(\"Number of Neighbors\")\nplt.xlabel(\"Test score\")\nplt.ylim((0.9, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this dataset, the functional observations have been sampled equiespaciated.\nIf we approximate the integral of the $\\mathbb{L}^2$ distance as a\nRiemann sum (actually the Simpson's rule it is used), we obtain that\nit is approximately equivalent to the euclidean distance between vectors.\n\n\\begin{align}\\|f - g \\|_{\\mathbb{L}^2} =  \\left ( \\int_a^b |f(x) - g(x)|^2 dx \\right )\n  ^{\\frac{1}{2}} \\approx \\left ( \\sum_{n=0}^{N}\\bigtriangleup h \\,|f(x_n)\n   - g(x_n)|^2 \\right ) ^ {\\frac{1}{2}}\\\\\n  = \\sqrt{\\bigtriangleup h} \\, d_{euclidean}(\\vec{f}, \\vec{g})\\end{align}\n\n\nSo, in this case, it is roughtly equivalent to use this metric instead of the\nfunctional one, due to the constant multiplication do no affect the\norder of the neighbors.\n\nSetting the parameter ``sklearn_metric`` of the classifier to True,\na vectorial metric of sklearn can be passed. In\n:class:`sklearn.neighbors.DistanceMetric` there are listed all the metrics\nsupported.\n\nWe will fit the model with the sklearn distance and search for the best\nparameter. The results can vary sightly, due to the approximation during\nthe integration, but the result should be similar.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(metric='euclidean', sklearn_metric=True)\ngscv2 = GridSearchCV(knn, param_grid, cv=KFold(shuffle=True, random_state=0))\ngscv2.fit(X, y)\n\nprint(\"Best params:\", gscv2.best_params_)\nprint(\"Best score:\", gscv2.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The advantage of use the sklearn metrics is the computational speed, three\norders of magnitude faster. But it is not always possible to resample samples\nequiespaced nor do all functional metrics have a vector equivalent in this\nway.\n\nThe mean score time depending on the metric is shown below.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Mean score time (seconds)\")\nprint(\"L2 distance:\", np.mean(gscv.cv_results_['mean_score_time']), \"(s)\")\nprint(\"Sklearn distance:\", np.mean(gscv2.cv_results_['mean_score_time']), \"(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This classifier can be used with multivariate funcional data, as surfaces\nor curves in $\\mathbb{R}^N$, if the metric support it too.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}