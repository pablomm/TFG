@incollection{eXtreme,
author = {Blankenship, J. and Bussa, M. and Millet, S.},
booktitle = {Pro Agile .NET Development with Scrum},
chapter = {3},
title = {{eXtreme Programming}},
year = {2011}
}
@misc{scipy,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
title = {{{SciPy}}: Open source scientific tools for {{Python}}},
url = {http://www.scipy.org/}
}
@article{dpa,
author = {Bertsekas, Dimitri},
title = {{Dynamic Programming and Optimal Control}},
year = {1995}
}
@article{berkeley,
author = {Jone, Harold E. and Bayley, Nancy},
journal = {Child Development},
number = {Jun},
pages = {167--173},
title = {{The Berkeley Growth Study}},
volume = {12},
year = {1941}
}
@article{Cencov1982,
author = {CÃªncov, N.},
journal = {Translations of Mathematical Monographs},
title = {{Statistical Decision Rules and Optimal Inferences}},
volume = {53},
year = {1982}
}
@misc{workshop,
title = {{III International Workshop on Advances in Functional Data Analysis}},
url = {https://www.iwafda3.unican.es/},
year = {2019}
}
@misc{fdasrvf,
author = {Tucker, J. Derek},
title = {{fdasrvf: Elastic Functional Data Analysis}},
year = {2017}
}
@misc{fda-r,
author = {Ramsay, James O. and Wickham, Hadley and Graves, Spencer and Hooker, Giles},
booktitle = {2018},
title = {{Cran R - fda package}}
}
@inproceedings{scikitfda,
author = {Ramos, Carlos},
booktitle = {III International Workshop on Advances in Functional Data Analysis},
file = {:Users/pablomm/Functional data analysis/Resources/Carlos{\_}Ramos.pdf:pdf},
title = {scikit-fda},
url = {https://www.iwafda3.unican.es/slides/Carlos{\_}Ramos.pdf},
year = {2019}
}
@article{FdaUsc,
author = {Febrero-Bande, Manuel},
file = {:Users/pablomm/Functional data analysis/Resources/v51i04.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {depth measures,estimation,functional data regression,non-parametric kernel,outlier,representation of functional data},
number = {4},
pages = {1--28},
title = {{Statistical Computing in Functional Data Analysis: The R Package fda.usc}},
volume = {51},
year = {2012}
}
@incollection{baillo2010,
abstract = {The aim of this chapter is to provide a survey of the literature concerning classifi- cation of functional data. Roughly half of the survey is devoted to supervised clas- sification and the rest to the unsupervised case. Whereas most available methods (k-NN, kernel, k-means, ...) are adaptations to the infinite-dimensional setting of other well-known methodologies from the classical multivariate framework, others (such as those based on random projections) have been especially developed for functional problems. Some practical issues, in particular real-data examples and simulations, are also reviewed. Some selected proofs are sketched.},
author = {Ba{\'{i}}llo, Amparo and Cuevas, Antonio and Fraiman, Ricardo},
booktitle = {The Oxford Handbook of Functional Data Analysis},
chapter = {10},
editor = {Ferraty, Fr{\'{e}}d{\'{e}}ric and Romain, Yves},
file = {:Users/pablomm/Functional data analysis/Resources/10-Ferraty-c10-drv.dvi.pdf:pdf},
pages = {259--297},
publisher = {Oxford University Press},
title = {{Classification methods for functional data}},
year = {2010}
}
@article{Kumar2008,
abstract = {Abstract. Many computer vision require searching a set of for , which is a very expensive operation. In this work, we compare and evaluate a number of for speeding up this task. Since follow},
author = {Kumar, Neeraj and Zhang, Li and Nayar, Shree},
doi = {10.1007/978-3-540-88688-4-27},
file = {:Users/pablomm/Functional data analysis/Resources/Kumar{\_}ECCV08{\_}2.pdf:pdf},
isbn = {3540886850},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {364--378},
title = {{What is a good nearest neighbors algorithm for finding similar patches in images?}},
volume = {5303 LNCS},
year = {2008}
}
@misc{scikit-fda2019,
author = {{Ramos Carre{\~{n}}o}, Carlos},
title = {{Scikit-fda project wiki}},
url = {https://github.com/GAA-UAM/scikit-fda/wiki},
urldate = {2005-09-20},
year = {2019}
}
@book{Ramsay2005,
abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
author = {Ramsay, James O. and Silverman, Bernard Walter},
booktitle = {Springer Series in Statistics},
doi = {10.1007/B98888},
edition = {2},
file = {:Users/pablomm/Functional data analysis/Resources/Functional{\_}Data{\_}Analysis{\_}2005{\_}Second{\_}Edition.pdf:pdf},
isbn = {9780387400808},
title = {{Functional Data Analysis}},
year = {2005}
}
@article{Kneip1983,
author = {Kneip, Alois and Gasser, Theo},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Kneip and Gasser 1992.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {Kneip1983},
number = {2},
pages = {416--431},
title = {{Statistical tools to analyze data representing a sample of curves}},
volume = {11},
year = {1983}
}
@article{Marron2015a,
abstract = {The abundance of functional observations in scientific endeavors has led to a significant development in tools for functional data analysis (FDA). This kind of data comes with several challenges: infinite-dimensionality of function spaces, observation noise, and so on. However, there is another interesting phenomena that creates problems in FDA. The functional data often comes with lateral displace-ments/deformations in curves, a phenomenon which is different from the height or amplitude variability and is termed phase variation. The presence of phase variability artificially often inflates data variance, blurs underlying data structures, and distorts principal components. While the separation and/or removal of phase from amplitude data is desirable, this is a difficult problem. In particular, a commonly used alignment procedure, based on minimizing the L 2 norm between functions , does not provide satisfactory results. In this paper we motivate the importance of dealing with the phase variability and summarize several current ideas for separating phase and amplitude components. These approaches differ in the following: (1) the definition and mathematical representation of phase variability, (2) the objective functions that are used in functional data alignment, and (3) the algorithmic tools for solving estimation/optimization problems. We use simple examples to illustrate various approaches and to provide useful contrast between them.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03216v1},
author = {Marron, J. S. and Ramsay, James O. and Sangalli, Laura M. and Srivastava, Anuj},
doi = {10.1214/15-sts524},
eprint = {arXiv:1512.03216v1},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/FunctionalDataAnalysisAmplitudPhaseVariation{\_}Marron{\_}Ramsay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Functional data analysis, registration, warping, a,alignment,and phrases,dynamic time warping,elastic metric,fisher,functional data analysis,ing,rao met-,registration,warp-},
number = {4},
pages = {468--484},
title = {{Functional Data Analysis of Amplitude and Phase Variation}},
volume = {30},
year = {2015}
}
@article{Marron2015,
abstract = {The abundance of functional observations in scientific endeavors has led to a significant development in tools for functional data analysis (FDA). This kind of data comes with several challenges: infinite-dimensionality of function spaces, observation noise, and so on. However, there is another interesting phenomena that creates problems in FDA. The functional data often comes with lateral displace-ments/deformations in curves, a phenomenon which is different from the height or amplitude variability and is termed phase variation. The presence of phase variability artificially often inflates data variance, blurs underlying data structures, and distorts principal components. While the separation and/or removal of phase from amplitude data is desirable, this is a difficult problem. In particular, a commonly used alignment procedure, based on minimizing the L 2 norm between functions , does not provide satisfactory results. In this paper we motivate the importance of dealing with the phase variability and summarize several current ideas for separating phase and amplitude components. These approaches differ in the following: (1) the definition and mathematical representation of phase variability, (2) the objective functions that are used in functional data alignment, and (3) the algorithmic tools for solving estimation/optimization problems. We use simple examples to illustrate various approaches and to provide useful contrast between them.},
author = {Marron, J. S. and Ramsay, James O. and Sangalli, Laura M. and Srivastava, Anuj},
doi = {10.1214/15-sts524},
file = {:Users/pablomm/Functional data analysis/Resources/Functional Data and Amplitude Phase Variation Marron 2015.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Functional data analysis,a,alignment,and phrases,dynamic time warping,elastic metric,fisher,functional data analysis,rao metric,registration,warping},
number = {4},
pages = {468--484},
title = {{Functional Data Analysis of Amplitude and Phase Variation}},
volume = {30},
year = {2015}
}
@article{RamsayAlois2008,
abstract = {A registration method can be defined as a process of aligning features of a sample of curves by monotone transformations of their domain. The aligned curves exhibit only amplitude variation, and the domain transformations, called warping functions, capture the phase variation in the original curves. In this article we precisely define a new type of registration process, in which the warping functions optimize the fit of a principal components decomposition to the aligned curves. The principal components are effectively the features that this process aligns. We discuss the relationship of registration to closure of a function space under convex operations, and define consistency for registration methods. We define an explicit decomposition of functional variation into amplitude and phase partitions, and develop an algorithm for combining registration with principal components analysis, and apply it to simulated and real data.},
author = {Kneip, Alois and Ramsay, James O.},
doi = {10.1198/016214508000000517},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Combining{\_}Registration{\_}and{\_}Fitting{\_}for{\_}Functional{\_}.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Continuous registration,Functional principal components analysis,Landmark registration,Phase variation,Time warping,Warping function},
number = {483},
pages = {1155--1165},
title = {{Combining registration and fitting for functional models}},
volume = {103},
year = {2008}
}
@article{Tucker2011,
abstract = {Constructing generative models for functional observations is an important task in statistical functional analysis. In general, functional data contains both phase (or x or horizontal) and amplitude (or y or vertical) variability. Traditional methods often ignore the phase variability and focus solely on the amplitude variation, using cross-sectional techniques such as fPCA for dimensional reduction and data modeling. Ignoring phase variability leads to a loss of structure in the data and inefficiency in data models. This paper presents an approach that relies on separating the phase (x-axis) and amplitude (y-axis), then modeling these components using joint distributions. This separation, in turn, is performed using a technique called elastic shape analysis of curves that involves a new mathematical representation of functional data. Then, using individual fPCAs, one each for phase and amplitude components, it imposes joint probability models on principal coefficients of these components while respecting the nonlinear geometry of the phase representation space. These ideas are demonstrated using random sampling, for models estimated from simulated and real datasets, and show their superiority over models that ignore phase-amplitude separation. Furthermore, the generative models are applied to classification of functional data and achieve high performance in applications involving SONAR signals of underwater objects, handwritten signatures, and periodic body movements recorded by smart phones.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.1791v2},
author = {Tucker, J. Derek and Wu, Wei and Srivastava, Anuj},
doi = {10.1016/j.csda.2012.12.001},
eprint = {arXiv:1212.1791v2},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Generatieve{\_}Models{\_}Tucker{\_}Srivastava.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Amplitude variability,Function alignment,Function principal component analysis,Functional data analysis,Generative model,Phase variability},
pages = {50--66},
title = {{Generative models for functional data using phase and amplitude separation}},
volume = {61},
year = {2013}
}
@phdthesis{FDA2018,
abstract = {Functional data analysis (FDA) is a field of Statistics in which the data considered are functions. This kind of data analysis takes into account that the underlying mechanism that generates the observations is a stochastic process. In this manner, it is possible to address some important questions (e.g. related to the continuity and smoothness of the functions measured) that tradi- tional multivariate analysis can not. Given that it is a relatively recent ?eld of study, the methods of FDA are not as widely known as those of multivariate analysis. Speci?cally, there are few software implementations of FDA tools available, mostly in MATLAB and R. However, these are not built following open-source schemes, which makes it di?cult to meke contributions to them. Up to this day there are not any packages for Functional Data Analysis in Python. Since Python is the language of choice for many data scientists and software developers, an implementation of FDA techniques in this programming language seems to be of some utility. Functional data analysis is a vast ?eld that keeps growing thanks to the work of numerous research teams around the world. That is why the aim of this project is not simply to design a FDA toolbox in Python but also to start an open-source project to which anybody can con- tribute. In this manner, the package can grow and be kept up to date even after the completion of this undergraduate thesis project. With these goals in mind, the fda Python project has been published in a public repository in GitHub. At the moment it includes basic FDA functionalities. In the long term we expect that it will become a comprehensive and widely-used toolbox for FDA.},
author = {Carbajo, Miguel},
file = {:Users/pablomm/Functional data analysis/Resources/TFG{\_}Miguel{\_}Carbajo.pdf:pdf},
keywords = {FDA,Functional data analysis,Python,software},
mendeley-tags = {Functional data analysis},
number = {May},
school = {Universidad Aut{\'{o}}noma de Madrid},
title = {{FDA-PY: Development of a python package for functional data analysis}},
year = {2018}
}
@book{Ferraty2006,
abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
author = {Ferraty, Fr{\'{e}}d{\'{e}}ric and Vieu, Philippe},
booktitle = {Springer Series in Statistics},
doi = {10.1007/0-387-36620-2},
file = {:Users/pablomm/Functional data analysis/Resources/Nonparametric{\_}Functional{\_}Data{\_}Analisis.pdf:pdf},
isbn = {9780387303697},
publisher = {Springer},
title = {{Nonparametric Functional Data Analysis}},
year = {2006}
}
@article{Gervini2004,
abstract = {This article introduces a semiparametricmodel for functional data. The warping func- tions are assumed to be linear combinations of q common components, which are estimated from the data (hence the name self-modeling). Even small values of q provide remarkable model flexibility, comparable to nonparametric methods. At the same time, this approach avoids overfitting because the common components are es- timated combining data across individuals. As a convenient by-product, component scores are often interpretable and can be used for statistical inference (an example of classification based on scores is given).},
author = {Gervini, Daniel and Gasser, Theo},
doi = {10.1111/j.1467-9868.2004.B5582.x},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/ Gervini{\_}Gasser{\_}2004{\_}.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Curve registration,Functional data analysis,Semiparametric models,Time warping},
number = {4},
pages = {959--971},
title = {{Self-modelling warping functions}},
volume = {66},
year = {2004}
}
@article{Ramsay1982,
author = {Ramsay, James O.},
file = {:Users/pablomm/Functional data analysis/Resources/When{\_}The{\_}Data{\_}Are{\_}Functions{\_}Ramsay.pdf:pdf},
journal = {Psychometrika},
keywords = {continuous data,duality diagram,functional analysis},
number = {4},
pages = {379--396},
title = {{When the data are functions}},
volume = {47},
year = {1982}
}
@misc{numpy,
  author =    {Travis Oliphant},
  title =     {{NumPy}: A guide to {NumPy}},
  year =      {2006--},
  howpublished = {USA: Trelgol Publishing},
  url = "http://www.numpy.org/",
 }
@article{sklearn,
abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
archivePrefix = {arXiv},
arxivId = {1309.0238},
author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"{e}}l},
eprint = {1309.0238},
file = {:Users/pablomm/Functional data analysis/Resources/1309.0238.pdf:pdf},
pages = {1--15},
title = {{API design for machine learning software: experiences from the scikit-learn project}},
url = {http://arxiv.org/abs/1309.0238},
year = {2013}
}
@article{Srivastava2011a,
abstract = {We introduce a novel geometric framework for separating the phase and the amplitude variability in functional data of the type frequently studied in growth curve analysis. This framework uses the Fisher-Rao Riemannian metric to derive a proper distance on the quotient space of functions modulo the time-warping group. A convenient square-root velocity function (SRVF) representation transforms the Fisher-Rao metric into the standard {\$}\backslashltwo{\$} metric, simplifying the computations. This distance is then used to define a Karcher mean template and warp the individual functions to align them with the Karcher mean template. The strength of this framework is demonstrated by deriving a consistent estimator of a signal observed under random warping, scaling, and vertical translation. These ideas are demonstrated using both simulated and real data from different application domains: the Berkeley growth study, handwritten signature curves, neuroscience spike trains, and gene expression signals. The proposed method is empirically shown to be be superior in performance to several recently published methods for functional alignment.},
archivePrefix = {arXiv},
arxivId = {1103.3817},
author = {Srivastava, Anuj and Wu, Wei and Kurtek, Sebastian and Klassen, Eric and Marron, J. S.},
eprint = {1103.3817},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Registration of Functional Data Using Fisher-Rao Metric.pdf:pdf},
number = {March},
title = {{Registration of Functional Data Using Fisher-Rao Metric}},
url = {http://arxiv.org/abs/1103.3817},
year = {2011}
}
@phdthesis{Tucker2014,
abstract = {Constructing generative models for functional observations is an important task in statistical func- tion analysis. In general, functional data contains both phase (or x or horizontal) and amplitude (or y or vertical) variability. Traditional methods often ignore the phase variability and focus solely on the amplitude variation, using cross-sectional techniques such as functional principal component analysis for dimensional reduction and regression for data modeling. Ignoring phase variability leads to a loss of structure in the data, and inefficiency in data models. Moreover, most methods use a âpre-processingâ alignment step to remove the phase-variability; without considering a more natural joint solution. This dissertation presents three approaches to this problem. The first relies on separating the phase (x-axis) and amplitude (y-axis), then modeling these components using joint distributions. This separation in turn, is performed using a technique called elastic alignment of functions that involves a new mathematical representation of functional data. Then, using individual principal components, one for each phase and amplitude components, it imposes joint probability models on principal coefficients of these components while respecting the nonlinear geometry of the phase representation space. The second combines the phase-variability into the objective function for two component analysis methods, functional principal component analysis and functional principal least squares. This creates a more complete solution, as the phase-variability is removed while simultaneously extracting the components. The third approach combines the phase-variability into the functional linear regression model and then extends the model to logistic and multinomial logistic regression. Through incorporating the phase-variability a more parsimonious regression model is obtained and therefore, more accurate prediction of observations is achieved. These models then are easily extended from functional data to curves (which are essentially functions in R2) to perform regression with curves as predictors. These},
author = {Tucker, J. Derek},
file = {:Users/pablomm/Functional data analysis/Resources/PDF datastream.pdf:pdf},
school = {Florida State University},
title = {{Functional Component Analysis and Regression Using Elastic Methods}},
year = {2014}
}
@book{Lee2018,
abstract = {These lecture notes grew out of an M.Sc. course on differential geometry which I gave at the University of Leeds 1992. Their main purpose is to introduce the beautiful theory of Riemannian Geometry a still very active area of mathematical research. This is a subject with no lack of interesting examples. They are indeed the key to a good understanding of it and will therefore play a major role throughout this work. Of special interest are the classical Lie groups allowing concrete calculations of many of the abstract notions on the menu. The},
author = {Lee, John M.},
doi = {10.1007/978-3-319-91755-9},
edition = {2},
file = {:Users/pablomm/Functional data analysis/Resources/2018{\_}Book{\_}IntroductionToRiemannianManifo.pdf:pdf},
isbn = {978-3-319-91754-2},
publisher = {Springer},
title = {{Introduction to Riemannian Manifolds}},
volume = {176},
year = {2018}
}
@article{Ramsay1998,
abstract = {Functional data analysis involves the extension of familiar statistical procedures such as principal components analysis, linear modelling and canonical correlation analysis to data where the raw observation xi is a function. An essential preliminary to a functional data analysis is often the registration or alignment of salient curve features by suitable monotone transformations hi of the argument t, so that the actual analyses are carried out on the values xi {\{}hi ( t ){\}}. This is referred to as dynamic time warping in the engineering literature. In effect, this conceptualizes variation among functions as being composed of two aspects: horizontal and vertical, or domain and range. A nonparametric function estimation technique is described for identifying the smooth monotone transformations hi and is illustrated by data analyses. A second-order linear stochastic differential equation is proposed to model these components of variation.},
author = {Ramsay, James O. and Li, Xiaochun},
doi = {10.1111/1467-9868.00129},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Li{\_}Ramsay{\_}1997{\_}Curve{\_}Registration.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Dynamic time warping,Geometric brownian motion,Monotone functions,Spline,Stochastic time,Time warping},
number = {2},
pages = {351--363},
title = {{Curve registration}},
volume = {60},
year = {1998}
}
@article{Earls2016,
abstract = {We propose a model for functional data registration that compares favorably to the best methods of functional data registration currently available. It also extends current inferential capabilities for unregistered data by providing a flexible probabilistic framework that 1) allows for functional prediction in the context of registration and 2) can be adapted to include smoothing and registration in one model. The proposed inferential framework is a Bayesian hierarchical model where the registered functions are modeled as Gaussian processes. To address the computational demands of inference in high-dimensional Bayesian models, we propose an adapted form of the variational Bayes algorithm for approximate inference that performs similarly to MCMC sampling methods for well-defined problems. The efficiency of the adapted variational Bayes (AVB) algorithm allows variability in a predicted registered, warping, and unregistered function to be depicted separately via bootstrapping. Temperature data related to the el-ni$\backslash${\~{}}no phenomenon is used to demonstrate the unique inferential capabilities for prediction provided by this model.},
archivePrefix = {arXiv},
arxivId = {1502.00552},
author = {Earls, Cecilia and Hooker, Giles},
eprint = {1502.00552},
file = {:Users/pablomm/Functional data analysis/Registration/Papers/Variational Bayes for Functional Data Registration, Smoothing, and Prediction.pdf:pdf},
keywords = {bayesian modeling,functional data,functional prediction,registration,smoothing},
pages = {1--54},
title = {{Variational Bayes for Functional Data Registration, Smoothing, and Prediction}},
url = {http://arxiv.org/abs/1502.00552},
year = {2016}
}
@book{Morettin2017,
author = {Morettin, Pedro A and Pinheiro, Alu{\'{i}}sio and Vidakovic, Brani},
file = {:Users/pablomm/Functional data analysis/Resources/2017{\_}Book{\_}WaveletsInFunctionalDataAnalys.pdf:pdf},
isbn = {9783319596228},
publisher = {Springer},
title = {{Wavelets in Functional Data Analysis}},
year = {2017}
}
@article{Boor2006,
abstract = {This book is based on the author's experience with calculations involving polynomial splines. It presents those parts of the theory which are especially useful in calculations and stresses the representation of splines as linear combinations of B-splines. After two chapters summarizing polynomial approximation, a rigorous discussion of elementary spline theory is given involving linear, cubic and parabolic splines. The computational handling of piecewise polynomial functions (of one variable) of arbitrary order is the subject of chapters VII and VIII, while chapters IX, X, and XI are devoted to B-splines. The distances from splines with fixed and with variable knots is discussed in chapter XII. The remaining five chapters concern specific approximation methods, interpolation, smoothing and least-squares approximation, the solution of an ordinary differential equation by collocation, curve fitting, and surface fitting. The present text version differs from the original in several respects. The book is now typeset (in plain TeX), the Fortran programs now make use of Fortran 77 features. The figures have been redrawn with the aid of Matlab, various errors have been corrected, and many more formal statements have been provided with proofs. Further, all formal statements and equations have been numbered by the same numbering system, to make it easier to find any particular item. A major change has occured in Chapters IX-XI where the B-spline theory is now developed directly from the recurrence relations without recourse to divided differences. This has brought in knot insertion as a powerful tool for providing simple proofs concerning the shape-preserving properties of the B-spline series.},
author = {de Boor, Carl R.},
doi = {10.2307/2006241},
file = {:Users/pablomm/Functional data analysis/Resources/A{\_}Practical{\_}Guide{\_}to{\_}Spline.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
number = {149},
pages = {325},
title = {{A Practical Guide to Splines.}},
volume = {34},
year = {2006}
}
@article{Boor1981,
author = {de Boor, Carl R.},
file = {:Users/pablomm/Functional data analysis/Resources/bsplbasic.pdf:pdf},
number = {September},
title = {{B(asic)-Spline Basics}},
volume = {3},
year = {1981}
}
@book{Srivastava2016,
abstract = {This textbook for courses on function data analysis and shape data analysis describes how to define, compare, and mathematically represent shapes, with a focus on statistical modeling and inference. It is aimed at graduate students in analysis in statistics, engineering, applied mathematics, neuroscience, biology, bioinformatics, and other related areas. The interdisciplinary nature of the broad range of ideas coveredâfrom introductory theory to algorithmic implementations and some statistical case studiesâis meant to familiarize graduate students with an array of tools that are relevant in developing computational solutions for shape and related analyses. These tools, gleaned from geometry, algebra, statistics, and computational science, are traditionally scattered across different courses, departments, and disciplines; Functional and Shape Data Analysis offers a unified, comprehensive solution by integrating the registration problem into shape analysis, better preparing graduate students for handling future scientific challenges. Recently, a data-driven and application-oriented focus on shape analysis has been trending. This text offers a self-contained treatment of this new generation of methods in shape analysis of curves. Its main focus is shape analysis of functions and curvesâin one, two, and higher dimensionsâboth closed and open. It develops elegant Riemannian frameworks that provide both quantification of shape differences and registration of curves at the same time. Additionally, these methods are used for statistically summarizing given curve data, performing dimension reduction, and modeling observed variability. It is recommended that the reader have a background in calculus, linear algebra, numerical analysis, and computation.},
author = {Srivastava, Anuj and Klassen, Eric},
booktitle = {Springer Series in Statistics},
doi = {10.1080/00224065.2017.11918007},
file = {:Users/pablomm/Functional data analysis/Resources/2016{\_}Book{\_}FunctionalAndShapeDataAnalysis.pdf:pdf},
isbn = {9781493940189},
issn = {0022-4065},
number = {4},
publisher = {Springer},
title = {{Functional and Shape Data Analysis}},
volume = {49},
year = {2016}
}
@book{Ferraty2011,
abstract = {This chapter reviews some of the circumstances in which screening processes are used for classifying large numbers of entities into two or more final classes. It discusses various types of entity, ranging from chemical compounds to human beings, according to their constancy or mutability of character between successive stages. It presents two main types of screening, polytomous and regarded. In practical circumstances, some combinations of these are used, but the mathematical consequence of such a combination is very complicated and improved statistical techniques are needed before realistic analysis is possible. The chapter also discusses some of the ideas on screening of chemical compounds for therapeutic activity developed by Davies and Dunnett. Compared with these are the screening of crop varieties, screening in education, and screening based upon a multivariate criterion such as is often used in animal breeding.},
booktitle = {Contributions to Statistics},
doi = {10.2307/2982146},
editor = {Ferraty, Fr{\'{e}}d{\'{e}}ric},
file = {:Users/pablomm/Functional data analysis/Resources/2011{\_}Book{\_}RecentAdvancesInFunctionalData.pdf:pdf},
isbn = {9783790827354},
issn = {00359238},
publisher = {Springer},
title = {{Recent Advances in Functional Data Analysis and Related Topics}},
year = {2011}
}
@article{Srivastava2011,
abstract = {We introduce a novel geometric framework for separating the phase and the amplitude variability in functional data of the type frequently studied in growth curve analysis. This framework uses the Fisher-Rao Riemannian metric to derive a proper distance on the quotient space of functions modulo the time-warping group. A convenient square-root velocity function (SRVF) representation transforms the Fisher-Rao metric into the standard {\$}\backslashltwo{\$} metric, simplifying the computations. This distance is then used to define a Karcher mean template and warp the individual functions to align them with the Karcher mean template. The strength of this framework is demonstrated by deriving a consistent estimator of a signal observed under random warping, scaling, and vertical translation. These ideas are demonstrated using both simulated and real data from different application domains: the Berkeley growth study, handwritten signature curves, neuroscience spike trains, and gene expression signals. The proposed method is empirically shown to be be superior in performance to several recently published methods for functional alignment.},
archivePrefix = {arXiv},
arxivId = {1103.3817},
author = {Srivastava, Anuj and Wu, Wei and Kurtek, Sebastian and Klassen, Eric and Marron, J. S.},
eprint = {1103.3817},
file = {:Users/pablomm/Functional data analysis/Resources/1103.3817.pdf:pdf},
number = {March},
title = {{Registration of Functional Data Using Fisher-Rao Metric}},
url = {http://arxiv.org/abs/1103.3817},
year = {2011}
}
@techreport{Lyche2005,
abstract = {In this first chapter we consider the following fundamental problem: Given a set of points in the plane, determine a smooth curve that approximates the points. The algorithm for determining the curve from the points should be well suited for implementation on a computer. That is, it should be efficient and it should not be overly sensitive to roundoff errors in the computations. We only consider methods that involve a relatively small number of elementary arithmetic operations; this ensures that the methods are efficient. The sensitivity of the methods to round-off errors is controlled by insisting that all the operations involved should amount to forming weighted averages of the given points. This has the added advantage that the constructions are geometrical in nature and easy to visualise. In Section 1.1 we discuss affine and convex combinations and the convex hull of a set of points, and relate these concepts to numerical stability (sensitivity to rounding errors), while in Section 1.2 we give a brief and very informal introduction to parametric curves. The first method for curve construction, namely polynomial interpolation, is introduced in Section 1.3. In Section 1.4 we show how to construct Bezier curves, and in Section 1.5 we generalise this construction to spline curves. At the outset, our construction of spline curves is geometrical in nature, but in Section 1.6 we show that spline curves can be written conveniently in terms of certain basis functions, namely B-splines. In the final section, we relate the material in this chapter to the rest of the book.},
author = {Lyche, Tom and Morken, Knut},
file = {:Users/pablomm/Functional data analysis/Resources/hele.pdf:pdf},
keywords = {Approximation Theory,Arc Length,B-splines,Bezier Curves,Convex Hull,Knot Insertion,Linear Algebra,Marsden's Identity,Quadratic Interpolation,Splines,Tensor Product,quasi-interpolant},
title = {{Spline Methods Draft}},
url = {http://www.uio.no/studier/emner/matnat/ifi/INF-MAT5340/v13/undervisningsmateriale/book.pdf{\%}5Cnhttp://www.uio.no/studier/emner/matnat/math/MAT-INF4170/v15/pensumliste/index.html},
year = {2005}
}
@misc{Srivastava,
author = {Srivastava, Anuj},
file = {:Users/pablomm/Functional data analysis/Resources/Lecture2{\_}Srivastava.pdf:pdf},
title = {{Elastic functional data analysis}}
}
@book{Ramsay2009,
author = {Ramsay, James O. and Hooker, Giles and Graves, Spencer},
doi = {10.1007/978-0-387-98185-7},
edition = {1},
file = {:Users/pablomm/Functional data analysis/Resources/Functional{\_}Data{\_}Analysis{\_}With{\_}R{\_}And{\_}Matlab{\_}2009.pdf:pdf},
isbn = {9780387981840},
publisher = {Springer},
title = {{Functional Data Analysis with R and MATLAB}},
year = {2009}
}
@book{Kokoszka2017,
author = {Kokoszka, Piotr and Reimherr, Matthew},
booktitle = {Texts in Statistical Science},
file = {:Users/pablomm/Functional data analysis/Resources/Kokoszka y Reimherr - Introduction to functional data analysis (2017, Chapman {\&} Hall {\_} CRC).pdf:pdf},
publisher = {Taylor {\&} Francis Group},
title = {{Introduction to Functional Data Analysis}},
year = {2017}
}
@phdthesis{Aguilera2009,
author = {Aguilera, Mar{\'{i}}a del Carmen},
file = {:Users/pablomm/Functional data analysis/Resources/Estimaci{\'{o}}n penalizada con datos funcionales.pdf:pdf},
school = {Universidad e Granada},
title = {{Estimaci{\'{o}}n penalizada con datos funcionales}},
year = {2009}
}
@article{Li2008,
abstract = {We suggest a classification and feature extraction method on functional data where the predictor variables are curves. The method, called functional segment discriminant analysis (FSDA), combines the classical linear discriminant analysis and support vector machine. FSDA is particularly useful for irregular functional data, characterized by spatial heterogeneity and local patterns like spikes. FSDA not only reduces the computation and storage burden by using a fraction of the spectrum, but also identifies important predictors and extracts features. FSDA is highly flexible, easy to incorporate information from other data sources and/or prior knowledge from the investigators. We apply FSDA to two public domain data sets and discuss the understanding developed from the study. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Li, Bin and Yu, Qingzhao},
doi = {10.1016/j.csda.2008.03.024},
file = {:Users/pablomm/Functional data analysis/Resources/Classification of functional data- A segmentation approach.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {10},
pages = {4790--4800},
title = {{Classification of functional data: A segmentation approach}},
volume = {52},
year = {2008}
}
@unpublished{ramos-carrenoScikitfdaPythonPackage2019,
  language = {english},
  venue = {{Castro Urdiales, Spain}},
  title = {Scikit-Fda: {{A Python}} Package for {{Functional Data Analysis}}},
  url = {https://www.iwafda3.unican.es/abstracts/Ramos_Carlos.pdf},
  abstract = {In this contribution we introduce scikit-fda (https://github.com/GAA-UAM/fda/), a Python library for the statistical analysis of functional data,  which is seamlessly integrated in the SciPy ecosystem.  To our knowledge,  this  is  the  first  software  package  that  allows  a  complete  treatment  of  these  types  of  data  in Python, one of foremost programming environments in the machine learning and data science communities. The library provides support for the analysis of functional data either in a discretized form or in a basis representation.  The scikit-fda Python package incorporates tools for registration, outlier detection, smoothing, exploratory analysis, and visualization of the data.  Other modules in the library are devoted to dimensionality reduction, clustering, functional regression with both scalar and functional responses, and classification. At the current stage, the library is being extended to include additional features and functionalities available in other programming languages (e.g.  R), as well as current developments in the area.  The library has been designed to be user friendly, flexible, efficient, and comprehensive, so as to fulfill the needs of statisticians, machine learning practitioners, and data scientists alike.},
  type = {Workshop},
  note = {{{III International Workshop}} on {{Advances}} in {{Functional Data Analysis}}},
  eventtitle = {{{III International Workshop}} on {{Advances}} in {{Functional Data Analysis}}},
  urldate = {2019-06-13},
  year = {2019},
  month = {05},
  date = {2019-05-24},
  author = {Ramos-CarreÃ±o, Carlos}
}
